{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels), (test_images, testlabels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x163e8cc6f98>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x163e8cc6f98>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(training_images[0])\n",
    "#print(training_labels[0])\n",
    "#print(training_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(training_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
      "    0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
      "   54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
      "  144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
      "  107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
      "  216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
      "  223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
      "  235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
      "  180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
      "  169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
      "  198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
      "  232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
      "  222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
      "  211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
      "  224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
      "  255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
      "  188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
      "  168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
      "  239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
      "  199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
      "  195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
      "  210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
      "  182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
      "    0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
      "   54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
      "  144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
      "  107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
      "  216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
      "  223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
      "  235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
      "  180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
      "  169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
      "  198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
      "  232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
      "  222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
      "  211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
      "  224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
      "  255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
      "  188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
      "  168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
      "  239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
      "  199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
      "  195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
      "  210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
      "  182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(training_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 23:07:55.454877  5816 deprecation.py:506] From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 23:07:55.454877  5816 deprecation.py:506] From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                   tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                   tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5000 - acc: 0.8251\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5000 - acc: 0.8251\n",
      "Epoch 2/5\n",
      "   32/60000 [..............................] - ETA: 11s - loss: 0.1015 - acc: 1.0000Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.3749 - acc: 0.8652\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.3749 - acc: 0.8652\n",
      "Epoch 3/5\n",
      "   32/60000 [..............................] - ETA: 14s - loss: 0.5100 - acc: 0.8125Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3371 - acc: 0.8774\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3371 - acc: 0.8774\n",
      "Epoch 4/5\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3102 - acc: 0.8872\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3102 - acc: 0.8872\n",
      "Epoch 5/5\n",
      "   32/60000 [..............................] - ETA: 11s - loss: 0.1449 - acc: 0.9375Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2933 - acc: 0.8910\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2933 - acc: 0.8910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163e510a6a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163e510a6a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.3614 - acc: 0.8734\n",
      "\b"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3614104016304016, 0.8734]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.3614 - acc: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3614104016304016, 0.8734]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.6877794e-06 1.4197528e-08 1.2534307e-06 1.8935804e-08 9.7671527e-06\n",
      " 4.7333385e-03 2.3473424e-06 3.2890946e-02 2.6617208e-05 9.6233106e-01]\n",
      "[4.6877794e-06 1.4197528e-08 1.2534307e-06 1.8935804e-08 9.7671527e-06\n",
      " 4.7333385e-03 2.3473424e-06 3.2890946e-02 2.6617208e-05 9.6233106e-01]\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(testlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.6877794e-06 1.4197528e-08 1.2534307e-06 1.8935804e-08 9.7671527e-06\n",
      " 4.7333385e-03 2.3473424e-06 3.2890946e-02 2.6617208e-05 9.6233106e-01]\n",
      "[4.6877794e-06 1.4197528e-08 1.2534307e-06 1.8935804e-08 9.7671527e-06\n",
      " 4.7333385e-03 2.3473424e-06 3.2890946e-02 2.6617208e-05 9.6233106e-01]\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(testlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                   tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                   tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   32/60000 [..............................] - ETA: 5:40 - loss: 2.9740Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5023\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5023\n",
      "Epoch 2/5\n",
      "   32/60000 [..............................] - ETA: 13s - loss: 0.3391Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3776\n",
      "\bEpoch 3/5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3776\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3387\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3387\n",
      "Epoch 4/5\n",
      "   32/60000 [..............................] - ETA: 14s - loss: 0.3431Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3151\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3151\n",
      "Epoch 5/5\n",
      "   32/60000 [..............................]Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.2989\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.2989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163e54590b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163e54590b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.3494\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.3494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34938603105545046"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.34938603105545046"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.7912322e-06 2.8577336e-08 2.4193082e-07 2.1569720e-10 7.5838261e-07\n",
      " 9.6084498e-02 4.2520132e-06 5.3830490e-02 4.2759137e-05 8.5002822e-01]\n",
      "[8.7912322e-06 2.8577336e-08 2.4193082e-07 2.1569720e-10 7.5838261e-07\n",
      " 9.6084498e-02 4.2520132e-06 5.3830490e-02 4.2759137e-05 8.5002822e-01]\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(testlabels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will hapeen if we add another laer between the one with 512 and the final layer with 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 12s 196us/sample - loss: 0.4687\n",
      "60000/60000 [==============================] - 12s 196us/sample - loss: 0.4687\n",
      "Epoch 2/5\n",
      "   32/60000 [..............................] - ETA: 18s - loss: 0.3503Epoch 2/5\n",
      "60000/60000 [==============================] - 12s 199us/sample - loss: 0.3566\n",
      "60000/60000 [==============================] - 12s 199us/sample - loss: 0.3566\n",
      "Epoch 3/5\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 11s 180us/sample - loss: 0.3206\n",
      "60000/60000 [==============================] - 11s 180us/sample - loss: 0.3206\n",
      "Epoch 4/5\n",
      "   32/60000 [..............................] - ETA: 16s - loss: 0.2197Epoch 4/5\n",
      "60000/60000 [==============================] - 12s 195us/sample - loss: 0.2949\n",
      "60000/60000 [==============================] - 12s 195us/sample - loss: 0.2949\n",
      "Epoch 5/5\n",
      "   32/60000 [..............................] - ETA: 28s - loss: 0.3853Epoch 5/5\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 0.2775\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 0.2775\n",
      "10000/10000 [==============================] - 1s 101us/sample - loss: 0.3435\n",
      "10000/10000 [==============================] - 1s 101us/sample - loss: 0.3435\n",
      "[2.2119864e-09 1.5269592e-08 3.5336387e-09 1.2375272e-08 1.9986460e-08\n",
      " 3.8369038e-04 1.5876161e-07 1.1383526e-02 1.1657867e-08 9.8823255e-01]\n",
      "9\n",
      "[2.2119864e-09 1.5269592e-08 3.5336387e-09 1.2375272e-08 1.9986460e-08\n",
      " 3.8369038e-04 1.5876161e-07 1.1383526e-02 1.1657867e-08 9.8823255e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, testlabels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(testlabels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the impact of training for more or less epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4996\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4996\n",
      "Epoch 2/30\n",
      "   32/60000 [..............................] - ETA: 13s - loss: 0.2909Epoch 2/30\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.3760\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.3760\n",
      "Epoch 3/30\n",
      "   32/60000 [..............................] - ETA: 13s - loss: 0.1924Epoch 3/30\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3354\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3354\n",
      "Epoch 4/30\n",
      "   32/60000 [..............................] - ETA: 16s - loss: 0.1440Epoch 4/30\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3122\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3122\n",
      "Epoch 5/30\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.2941\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.2941\n",
      "Epoch 6/30\n",
      "   32/60000 [..............................] - ETA: 14s - loss: 0.2640Epoch 6/30\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.2806\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.2806\n",
      "Epoch 7/30\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2664\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2664\n",
      "Epoch 8/30\n",
      "   32/60000 [..............................] - ETA: 14s - loss: 0.1226Epoch 8/30\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2569\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2569\n",
      "Epoch 9/30\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.2481\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.2481\n",
      "Epoch 10/30\n",
      "   32/60000 [..............................] - ETA: 11s - loss: 0.3781Epoch 10/30\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2415\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2415\n",
      "Epoch 11/30\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2314\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2314\n",
      "Epoch 12/30\n",
      "   32/60000 [..............................] - ETA: 14s - loss: 0.3643Epoch 12/30\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2223\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2223\n",
      "Epoch 13/30\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2162\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2162\n",
      "Epoch 14/30\n",
      "   32/60000 [..............................] - ETA: 17s - loss: 0.0667Epoch 14/30\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2123\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2123\n",
      "Epoch 15/30\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2042\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2042\n",
      "Epoch 16/30\n",
      "   32/60000 [..............................] - ETA: 10s - loss: 0.2726Epoch 16/30\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.1977\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.1977\n",
      "Epoch 17/30\n",
      "   32/60000 [..............................] - ETA: 15s - loss: 0.3127Epoch 17/30\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.1926\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.1926\n",
      "Epoch 18/30\n",
      "   32/60000 [..............................] - ETA: 13s - loss: 0.0654Epoch 18/30\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1885\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1885\n",
      "Epoch 19/30\n",
      "   32/60000 [..............................] - ETA: 16s - loss: 0.0750Epoch 19/30\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.1832\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.1832\n",
      "Epoch 20/30\n",
      "   32/60000 [..............................] - ETA: 9s - loss: 0.1199Epoch 20/30\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.1772\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.1772\n",
      "Epoch 21/30\n",
      "   32/60000 [..............................] - ETA: 12s - loss: 0.1382Epoch 21/30\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.1731\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.1731\n",
      "Epoch 22/30\n",
      "   32/60000 [..............................] - ETA: 12s - loss: 0.1588Epoch 22/30\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.1683\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.1683\n",
      "Epoch 23/30\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.1651\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.1651\n",
      "Epoch 24/30\n",
      "   32/60000 [..............................] - ETA: 10s - loss: 0.3322Epoch 24/30\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1619\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1619\n",
      "Epoch 25/30\n",
      "   32/60000 [..............................] - ETA: 14s - loss: 0.1231Epoch 25/30\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1564\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1564\n",
      "Epoch 26/30\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1542\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1542\n",
      "Epoch 27/30\n",
      "   32/60000 [..............................] - ETA: 12s - loss: 0.1200Epoch 27/30\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.1517\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.1517\n",
      "Epoch 28/30\n",
      "   32/60000 [..............................] - ETA: 12s - loss: 0.0289Epoch 28/30\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.1464\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.1464\n",
      "Epoch 29/30\n",
      "   32/60000 [..............................] - ETA: 12s - loss: 0.1556Epoch 29/30\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1450\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1450\n",
      "Epoch 30/30\n",
      "   32/60000 [..............................] - ETA: 14s - loss: 0.0552Epoch 30/30\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1418\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1418\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.4046\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.4046\n",
      "[2.6830650e-18 1.4124256e-26 2.1926435e-20 7.4472587e-24 9.1914007e-17\n",
      " 1.3790885e-22 3.4700432e-23 6.1733164e-30 1.0000000e+00 2.4643998e-35]\n",
      "8\n",
      "[2.6830650e-18 1.4124256e-26 2.1926435e-20 7.4472587e-24 9.1914007e-17\n",
      " 1.3790885e-22 3.4700432e-23 6.1733164e-30 1.0000000e+00 2.4643998e-35]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=30)\n",
    "\n",
    "model.evaluate(test_images, testlabels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[34])\n",
    "print(testlabels[34])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you trained, we normalized the data, going from values that were 0-255 to values that were 0-1. What would be the impact of removing that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 162us/sample - loss: 0.2011\n",
      "60000/60000 [==============================] - 10s 162us/sample - loss: 0.2011\n",
      "Epoch 2/5\n",
      "   32/60000 [..............................] - ETA: 17s - loss: 0.0465Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.0807\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.0807\n",
      "Epoch 3/5\n",
      "   32/60000 [..............................] - ETA: 17s - loss: 0.0057Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.0513\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.0513\n",
      "Epoch 4/5\n",
      "   32/60000 [..............................] - ETA: 14s - loss: 0.1449Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.0350\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.0350\n",
      "Epoch 5/5\n",
      "   32/60000 [..............................] - ETA: 17s - loss: 0.0525Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 142us/sample - loss: 0.0275\n",
      "60000/60000 [==============================] - 9s 142us/sample - loss: 0.0275\n",
      "10000/10000 [==============================] - 1s 80us/sample - loss: 0.0714\n",
      "10000/10000 [==============================] - 1s 80us/sample - loss: 0.0714\n",
      "[8.3793577e-09 8.4771240e-10 3.1470847e-07 7.2635971e-06 1.5314738e-12\n",
      " 2.7934561e-09 1.0861137e-13 9.9998617e-01 4.4038528e-09 6.2537524e-06]\n",
      "7\n",
      "[8.3793577e-09 8.4771240e-10 3.1470847e-07 7.2635971e-06 1.5314738e-12\n",
      " 2.7934561e-09 1.0861137e-13 9.9998617e-01 4.4038528e-09 6.2537524e-06]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images/255.0\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "model.evaluate(test_images, test_labels)\n",
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 166us/sample - loss: 0.4749\n",
      "60000/60000 [==============================] - 10s 166us/sample - loss: 0.4749\n",
      "Epoch 2/5\n",
      "   32/60000 [..............................] - ETA: 17s - loss: 0.2632Epoch 2/5\n",
      "59712/60000 [============================>.] - ETA: 0s - loss: 0.3599\n",
      "Reached 60% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 10s 162us/sample - loss: 0.3598\n",
      "\n",
      "Reached 60% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 10s 162us/sample - loss: 0.3598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163871e2c18>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163871e2c18>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('loss')<0.4):\n",
    "      print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images/255.0\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
